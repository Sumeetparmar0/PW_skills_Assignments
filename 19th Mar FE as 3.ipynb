{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c688552-5c76-4730-b33d-eb143ae256ec",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a3a84-53df-418c-a2d5-8e9b97beb736",
   "metadata": {},
   "source": [
    "ANS:\n",
    "Min-Max Scaling:\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features into a specific range. The transformation scales the values between a minimum and maximum value, typically 0 and 1. This is achieved by applying the following formula to each data point:\n",
    "\n",
    "X scaled = (Xi - Xmin)/ (Xmax- Xmin)\n",
    "\n",
    "where Xi is the original data point, Xmin is the minimum value in the feature,\n",
    "Xmax is the maximum value in the feature, X scaled is the scaled (normalized) vlaue.\n",
    "\n",
    "Example:\n",
    "Let's consider a dataset of ages (in years) with values ranging from 20 to 40. We want to apply Min-Max scaling to normalize these ages to the range [0, 1].\n",
    "\n",
    "X scaled = Xi -20 / 40-20 so X scaled will be 0.5 when we havig Xi=30\n",
    "\n",
    "therefore , the age 30 would be normalized to 0.5 using min max scaling.\n",
    "\n",
    "Min-Max scaling is particularly useful when the features have different ranges, ensuring that all features contribute equally to the model's training. It's a common step in preparing data for machine learning algorithms, especially those sensitive to the scale of input features, such as neural networks and support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d537a86-87fb-4031-b0e3-2d2222d9e2cc",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "attachments": {
    "8df56321-ec3e-4728-a2a9-ef948413fc74.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a382f4f0-9f16-47e4-93dd-f6c19276b7d9",
   "metadata": {},
   "source": [
    "ANS:\n",
    "Unit Vector (Vector Normalization):\n",
    "\n",
    "The unit vector technique in feature scaling involves scaling individual data points to have a length of 1, transforming them into vectors of unit magnitude. This is achieved by dividing each data point by its Euclidean norm (magnitude). The formula for unit vector transformation is:\n",
    "\n",
    "\n",
    "![image.png](attachment:8df56321-ec3e-4728-a2a9-ef948413fc74.png)\n",
    "\n",
    "\n",
    "Example:\n",
    "Consider a dataset with two features, X1 and X2 , and tdata point is [3, 4].the eclidean norm of this vector is calculated as \n",
    "||X|| = sq root of 3^2 + 4^2 = 5\n",
    "\n",
    "Now, to obtain the unit vector, each component is divided by the Euclidean norm:\n",
    "\n",
    "X sclaed = [3/5, 4,5]\n",
    "The resulting unit vector has a magnitude of 1.\n",
    "\n",
    "Differences from Min-Max Scaling:\n",
    "\n",
    "Range: Min-Max scaling transforms data points into a specific range (e.g., [0, 1]), while unit vector scaling normalizes each data point to have a magnitude of 1.\n",
    "\n",
    "Direction: Unit vector scaling preserves the direction of the data points while adjusting their magnitude. Min-Max scaling, on the other hand, may alter the direction if the original range is not symmetric.\n",
    "\n",
    "Magnitude: In unit vector scaling, the magnitude is fixed at 1. In Min-Max scaling, the magnitude depends on the original range of the data.\n",
    "\n",
    "Unit vector scaling is useful when the direction of the data points is important, such as in clustering algorithms where the relative angles between data points matter more than their distances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c5b2db-dfd7-4a28-882e-6dc5570fb721",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf9f96-bf6a-4f01-aad1-159cb6d3d661",
   "metadata": {},
   "source": [
    "ANS\n",
    "PCA (Principal Component Analysis):\n",
    "\n",
    "PCA is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining as much of the original variability as possible. It does so by identifying the principal components, which are linear combinations of the original features.\n",
    "\n",
    "Steps in PCA:\n",
    "\n",
    "Standardization: Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "Covariance Matrix: Compute the covariance matrix for the standardized data.\n",
    "\n",
    "Eigendecomposition: Find the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "Select Principal Components: Sort the eigenvalues in descending order and choose the top \n",
    "k eigenvectors corresponding to thelargest eigenvalues, where k is the desired dimensionality of the reduced space.\n",
    "\n",
    "Projection: Project the original data onto the selected eigenvectors to obtain the new, lower-dimensional representation.\n",
    "\n",
    "Example:\n",
    "Consider a dataset with two feature, X1 and X2 ,\n",
    "After standardization and covariance matrix computation, the eigendecomposition yields eigenvaluers 位1 and 位2 along with corresponding eigenvectors v1 and v2.\n",
    "\n",
    "suppose 位1 > 位2 indicating that the first principal component PC1 captures more variace than the second PC2, by projectiong the data onto PC1 , we retain the most important info in a one dimentional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd4f87-749a-4a66-87de-abc2095ca88b",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea913e-f5bc-4620-ae05-e41614a383b8",
   "metadata": {},
   "source": [
    "ANS:\n",
    "Relationship between PCA and Feature Extraction:\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique commonly used for feature extraction. In the context of feature extraction, PCA identifies and retains the most important features (principal components) while discarding less important ones. The idea is to reduce the dimensionality of the data while preserving as much of its original variability as possible.\n",
    "\n",
    "How PCA is Used for Feature Extraction:\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: PCA computes the eigenvectors and eigenvalues of the covariance matrix of the standardized data.\n",
    "\n",
    "Select Principal Components: The eigenvectors corresponding to the largest eigenvalues represent the principal components. These components capture the directions in which the data varies the most.\n",
    "\n",
    "Projection: The data is projected onto the selected principal components, creating a new feature space. The new features are linear combinations of the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77703a4b-794e-499a-9a7b-aedbfe062580",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "Consider a dataset with multiple features. We can use PCA for feature extraction to reduce the dimensionality of the data while retaining the most important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f966752-75fc-4641-b24d-b3fe1afc63c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Reduced data:\n",
      " [[-5.19615242e+00  2.56395025e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.19615242e+00  2.56395025e-16]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example data with three features\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# PCA for feature extraction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Original data:\\n\", X)\n",
    "print(\"Reduced data:\\n\", X_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f6c09f-3dea-42c0-9613-cc3737929dba",
   "metadata": {},
   "source": [
    "In this example, PCA is applied to reduce the three-dimensional data to two dimensions. The new features in X_pca capture the most important information in the original data. This reduced representation is particularly useful when dealing with high-dimensional datasets or when trying to visualize the data in a lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd9161-8093-43cc-9d97-da620ddb6fda",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e378e-bbb4-4a06-af1f-a204532d0e70",
   "metadata": {},
   "source": [
    "ANS:\n",
    "Using Min-Max Scaling for Data Preprocessing in a Recommendation System:\n",
    "\n",
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be employed to standardize or normalize the features like price, rating, and delivery time. The goal is to transform these features so that they fall within a specific range, typically [0, 1], making them comparable and preventing one feature from dominating due to its scale. Here's how you might use Min-Max scaling:\n",
    "\n",
    "Understand the Data Range:\n",
    "\n",
    "For each feature (e.g., price, rating, delivery time), assess the range of values. Different features might have different scales.\n",
    "Apply Min-Max Scaling:\n",
    "\n",
    "Use the Min-Max scaling formula for each feature \n",
    "this scales the values to the range [0, 1].\n",
    "\n",
    "Implement in Python:\n",
    "\n",
    "Use a library like scikit-learn to apply Min-Max scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8beccf-ce3b-40c0-9a74-b11d992ac13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[10, 4.5, 30], [20, 3.8, 45], [15, 4.2, 20]]\n",
      "Scaled Data:\n",
      " [[0.         1.         0.4       ]\n",
      " [1.         0.         1.        ]\n",
      " [0.5        0.57142857 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example dataset with features: price, rating, delivery time\n",
    "data = [[10, 4.5, 30], [20, 3.8, 45], [15, 4.2, 20]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Scaled Data:\\n\", data_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f990e50-5e73-4c72-ab8e-62e741e01e4d",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "After scaling, each feature will have values between 0 and 1, making them comparable. This is crucial for algorithms that rely on distance metrics, such as collaborative filtering used in recommendation systems.\n",
    "Effect on Recommendation Model:\n",
    "\n",
    "Min-Max scaling ensures that features with different scales contribute equally to the recommendation model, preventing biases based on the scale of the features.\n",
    "By applying Min-Max scaling, you create a standardized representation of the features, contributing to the robustness and effectiveness of the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbdf08c-2e56-434c-889b-2f1dcda08170",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0ba7c-f97b-42f4-aedb-316059a0f014",
   "metadata": {},
   "source": [
    "ANS:\n",
    "Using PCA for Dimensionality Reduction in Stock Price Prediction:\n",
    "\n",
    "In the context of building a model to predict stock prices, Principal Component Analysis (PCA) can be a valuable tool for reducing the dimensionality of the dataset. Here's how you might use PCA:\n",
    "\n",
    "Understand the High-Dimensional Dataset:\n",
    "\n",
    "In stock price prediction, datasets often include numerous features, such as financial metrics, market trends, and economic indicators. This can result in a high-dimensional dataset.\n",
    "Apply PCA:\n",
    "\n",
    "PCA is used to transform the original features into a new set of uncorrelated features called principal components. These components are ordered by the amount of variance they capture. By selecting a subset of principal components, you can achieve dimensionality reduction.\n",
    "Steps to Apply PCA in Python:\n",
    "\n",
    "Import necessary libraries and load the dataset.\n",
    "Standardize the features if they are on different scales.\n",
    "Apply PCA to obtain the principal components.\n",
    "Decide on the number of components to keep based on explained variance.\n",
    "Transform the dataset using the selected principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51417430-c6c6-4afc-b792-7381fea570d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: (3, 3)\n",
      "Reduced Shape: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=3)  # Choose the number of components\n",
    "data_pca = pca.fit_transform(data_standardized)\n",
    "\n",
    "print(\"Original Shape:\", data.shape)\n",
    "print(\"Reduced Shape:\", data_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ca70e-aaa4-46fd-b526-976d20e9b1ea",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "The new dataset (data_pca) has a reduced number of features (dimensions) compared to the original dataset.\n",
    "Benefits of Dimensionality Reduction:\n",
    "\n",
    "Computational Efficiency: Reducing dimensions can speed up training and testing processes.\n",
    "Improved Model Performance: Removing irrelevant or redundant features can enhance model performance and reduce overfitting.\n",
    "Visualization: It becomes easier to visualize and understand the data when working with fewer dimensions.\n",
    "Effect on Stock Price Prediction Model:\n",
    "\n",
    "The reduced dataset can be used as input for building predictive models. The selected principal components capture the most significant variance in the data.\n",
    "Applying PCA helps address the curse of dimensionality, simplifying the dataset while retaining crucial information for predicting stock prices. It is important to strike a balance between dimensionality reduction and preserving predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c5b68-b487-4d30-b2b1-4076d76d7a27",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724eba5d-5f45-47b3-86bb-b150ad838716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Your dataset\n",
    "data = [[1], [5], [10], [15], [20]]\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd3ff6d-c658-45d2-b36d-a9a45d91e66a",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395fa1b8-995a-4a65-9a72-57e9cd6b48e7",
   "metadata": {},
   "source": [
    "ANS:\n",
    "The decision of how many principal components (PCs) to retain in PCA is often based on the cumulative explained variance. Here's a general approach:\n",
    "\n",
    "Calculate Explained Variance:\n",
    "\n",
    "Run PCA on your dataset.\n",
    "For each principal component, calculate the proportion of the variance it explains.\n",
    "Decide on a Threshold:\n",
    "\n",
    "Choose a threshold for the cumulative explained variance you want to retain. A common choice is often 95% or 99%.\n",
    "The threshold represents the amount of variance you want your retained principal components to explain.\n",
    "Select Principal Components:\n",
    "\n",
    "Add up the explained variances of the principal components until you reach or exceed the chosen threshold.\n",
    "The number of principal components at this point is the number you retain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa402c1-6c67-4fc2-a6ed-f23ae94bcee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's a Python example using sklearn:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'data' is your dataset\n",
    "data = [[height1, weight1, age1, gender1, bp1],\n",
    "        [height2, weight2, age2, gender2, bp2]# ... more data ... \n",
    "       ]\n",
    "        \n",
    "\n",
    "# Create a PCA instance\n",
    "pca = PCA()\n",
    "\n",
    "# Fit on data\n",
    "pca.fit(data)\n",
    "\n",
    "# Get explained variances\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "# Decide on a threshold, e.g., 95%\n",
    "threshold = 0.95\n",
    "\n",
    "# Find the number of components to retain\n",
    "num_components_to_retain = len(cumulative_explained_variance[cumulative_explained_variance < threshold]) + 1\n",
    "\n",
    "print(f\"Number of components to retain: {num_components_to_retain}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c3334-a040-4258-bb22-a2da35c4e7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
